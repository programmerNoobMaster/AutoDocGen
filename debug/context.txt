[the main.py:9-100] def main():
    parser = argparse.ArgumentParser(description="Generate technical documentation")
    parser.add_argument("--repo", dest="repo", default=None, required=False,
                        help="GitHub repo URL (or set REPO_URL env variable)")
    args = parser.parse_args()

    # Prefer CLI argument, then environment variable
    repo_url = args.repo or os.environ.get("REPO_URL")
    if not repo_url:
        print("ERROR: Provide a repo via --repo <url> (or set REPO_URL).", file=sys.stderr)
        sys.exit(2)

    # --- pipeline ---
    repo_path = clone_repo(repo_url)
    chunks = extract_all_chunks(repo_path)
    stats = save_to_faiss_split_by_ext(chunks, base_dir="docs_index", 

[the save_to_vector_db.py:20-22] def _norm_ext(ext: str) -> str:
    ext = (ext or "").lower().lstrip(".")
    return ext

[the graph.py:335-362] def decide_pass_or_revise(state: State):
    """
    Route based on judge result (and human notes if present).
    - If human left notes → revise once.
    - If judge flags problems or low score → revise up to max_retries.
    - Otherwise → save.
    """
    # Human path: if notes exist, do exactly one revise pass
    if state.get("_human_notes"):
        return "revise"

    # LLM path
    try:
        data = json.loads(state.get("_judge", "") or "{}")
    except Exception:
        return "revise"  # malformed judge → try revise once

    score = float(data.get("score", 0))
    bad = (not data.get("factual", True)) or data.get("hallucinated", False) or (not data.get("cites_ok", True))

    

[the streamlit_app.py:128-131] def humanize_stem(stem: str) -> str:
    """objective_and_scope -> Objective And Scope (best-effort)."""
    words = re.split(r'[_\-]+', stem)
    return " ".join(w.capitalize() for w in words if w)

[the streamlit_app.py:109-110] def _squeeze_blank_lines(md: str) -> str:
    return re.sub(r'\n{3,}', '\n\n', md).strip()

[the context.txt:1-124] [the run-pipeline.py:7-50] def main():
    p = argparse.ArgumentParser()
    p.add_argument("--repo", required=True, help="GitHub repo URL to analyze")
    p.add_argument("--embed", default="text-embedding-3-small")
    args = p.parse_args()

    # 1) clone -> 2) chunk -> 3) index
    repo_path = clone_repo(args.repo)
    chunks = extract_all_chunks(repo_path)
    save_to_faiss_split_by_ext(chunks, base_dir="docs_index", model=args.embed)

    # 4) run your graph to write Markdown into app/docs/
    app = build_graph()

    def run(spec: SectionSpec):
        out = app.invoke({"spec": spec})
        print("Wrote:", out["out_path"], flush=True)

    # Use the same sections you generate now (a

[the streamlit_app.py:173-226] Calls app/main.py with --repo <url>, streams logs, and collects file paths
printed as 'Wrote: <path>'. Windows-safe; prefers venv Python.

[the graph.py:122-173] Code-first retrieval (when route='both'), per-file cap, char budget, strict [the file:lines] tags.

[the requirement.txt:1-13] openai
langchain
langgraph
langchain-openai
langchain-community
python-dotenv
faiss-cpu
gitpython
nbformat
pandas
python-docx
tiktoken
